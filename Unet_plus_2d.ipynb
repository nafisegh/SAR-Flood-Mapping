{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#from sklearn.decomposition import PCA\n",
    "import pickle\n",
    "from keras_unet_collection import models\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow\n",
    "import pandas as pd\n",
    "from keras_unet_collection import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_plus_2d = models.unet_plus_2d((512,512,3), [64,128,256,512], n_labels=2,\n",
    "                    stack_num_down=2, stack_num_up=2,\n",
    "                    activation='LeakyReLU', output_activation='Softmax',\n",
    "                    batch_norm=False, pool='max', unpool=False, deep_supervision=True, name='xnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_plus_2d.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FOLD = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_image_mask_generator(image_data_generator, mask_data_generator):\n",
    "    train_generator = zip(image_data_generator, mask_data_generator)\n",
    "    for (img, mask) in train_generator:\n",
    "        yield (img, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale =1, shear_range = 0, zoom_range=1,\n",
    "                                    horizontal_flip = False, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE,EPOCHS =1, 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule=ExponentialDecay(0.01,decay_rate=0.8,decay_steps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,6):\n",
    "\n",
    "        #j+=1\n",
    "        \n",
    "        x_train_df = pd.read_csv(f'G:/folds_new/train_fold_{i}.csv')\n",
    "        \n",
    "        x_valid_df = pd.read_csv(f'G:/folds_new/valid_fold_{i}.csv')\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "         \n",
    "        training_set = train_datagen.flow_from_dataframe(dataframe= x_train_df, directory=f'G:/sen11_cross_validation_image/fold_{i}/train',\n",
    "                                                 x_col=\"Image\", \n",
    "                                                 class_mode=None,\n",
    "                                                 target_size=(512,512), batch_size=BATCH_SIZE,\n",
    "                                                 seed = 1)\n",
    "                                                 \n",
    "                                                 \n",
    "        \n",
    "        training_mask = train_datagen.flow_from_dataframe(dataframe = x_train_df,\n",
    "                                                  \n",
    "                                                  directory = f'G:/sen11_cross_validation_mask_editted_2/fold_{i}/train',\n",
    "                                                  x_col = \"Mask\", \n",
    "                                                  target_size = (512,512),\n",
    "                                                  class_mode = None,\n",
    "                                                  batch_size = BATCH_SIZE,\n",
    "                                                  color_mode = \"grayscale\",\n",
    "                                                  seed = 1)\n",
    "                                                  \n",
    "        validation_set = train_datagen.flow_from_dataframe(dataframe = x_valid_df, directory = f'G:/sen11_cross_validation_image/fold_{i}/valid',\n",
    "        \n",
    "                                                      x_col=\"Image\",\n",
    "                                                      class_mode=None,\n",
    "                                                      target_size=(512,512), batch_size=BATCH_SIZE,\n",
    "                                                      seed=1)  \n",
    "                                                      \n",
    "        validation_mask = train_datagen.flow_from_dataframe( dataframe = x_valid_df, directory = f'G:/sen11_cross_validation_mask_editted_2/fold_{i}/valid',\n",
    "                                                        x_col = \"Mask\",\n",
    "                                                        class_mode=None,\n",
    "                                                        target_size=(512,512), batch_size=BATCH_SIZE,\n",
    "                                                        color_mode=\"grayscale\",\n",
    "                                                        seed=1)\n",
    "         \n",
    "         \n",
    "         \n",
    "                                           \n",
    "        training_set_generator = my_image_mask_generator(training_set, training_mask)   \n",
    "        \n",
    "        validation_set_generator = my_image_mask_generator(validation_set, validation_mask)             \n",
    "        \n",
    "              \n",
    "        unet_plus_2d.compile(loss='binary_crossentropy',optimizer=Adam(learning_rate=lr_schedule),metrics=['accuracy'])\n",
    "      \n",
    "        #unet_plus_2d=load_model('D:/Nafiseh/flood_proposal/QC_2019_new_train_data_unet_plus_2d.h5')\n",
    "      \n",
    "      \n",
    "        es=EarlyStopping(monitor='val_loss',patience=10,restore_best_weights=True)\n",
    "      \n",
    "        mch=ModelCheckpoint(filepath=f'G:/Sen1Flood11_data_Unet_plus_2d_fold_{i}.h5',save_best_only=True)\n",
    "      \n",
    "        #history_unet_plus_2d=unet_plus_2d.fit(train_resampled__,train_label_unet,batch_size,epochs=epochs,callbacks=[es,mch],validation_data=(val_\n",
    "      \n",
    "        history=unet_plus_2d.fit_generator(training_set_generator,\n",
    "                                          epochs = EPOCHS,\n",
    "                                          steps_per_epoch=x_train_df.shape[0] // BATCH_SIZE, callbacks=[es, mch], validation_steps = x_valid_df.shape[0], \n",
    "                                          validation_data=validation_set_generator)\n",
    "      \n",
    "      \n",
    "        #predicted_segments=np.argmax(unet_plus_2d.predict(test_resampled__)[-1],axis=3)\n",
    "      \n",
    "        #gc.collect()\n",
    "      \n",
    "        with open(f'G:/training_history_Unet_Plus_2d_Sen1Flood11_data_fold_{i}.pckl','wb') as hist:\n",
    "      \n",
    "            pickle.dump(history.history,hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "for i in range(1,6):\n",
    "    \n",
    "      unet_plus_2d=load_model(f'G:/Sen1Flood11_data_Unet_plus_2d_fold_{i}.h5')  \n",
    "    \n",
    "      x_test_df = pd.read_csv(f'G:/folds_new/test_fold_{i}.csv')\n",
    "    \n",
    "    \n",
    "      test_set = test_datagen.flow_from_dataframe(dataframe=x_test_df, directory=f'G:/sen11_cross_validation_image/fold_{i}/test',\n",
    "                                                 x_col=\"Image\",\n",
    "                                                 class_mode=None,\n",
    "                                                 target_size=(512,512), batch_size=BATCH_SIZE,\n",
    "                                                 seed =1)\n",
    "      \n",
    "      predicted = np.argmax(unet_plus_2d.predict(test_set, x_test_df.shape[0]//BATCH_SIZE)[0],axis=3) \n",
    "      np.save(f'G:/predicted_Sen1Flood11_Unet_Plus_2d_fold_{i}.npy',predicted)\n",
    "      \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from osgeo import gdal\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = gdal.Open('G:/nafise_bucket/Pakistan/Pakistan_9684_Pre.tif')\n",
    "\n",
    "image = img.GetRasterBand(2)\n",
    "\n",
    "image_ = image.ReadAsArray().astype('float')\n",
    "\n",
    "plt.imshow(image_)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(np.min(image_), np.max(image_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
