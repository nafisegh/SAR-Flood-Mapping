{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(314, 303, 4) (473, 633, 4)\n"
     ]
    }
   ],
   "source": [
    "from osgeo import gdal\n",
    "import numpy as np\n",
    "from scipy.ndimage import zoom\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def read_as_array(path,file_name):\n",
    "\n",
    "    gdal_file=gdal.Open(path+'/'+file_name)\n",
    "\n",
    "    array=np.zeros((gdal_file.RasterYSize,gdal_file.RasterXSize,gdal_file.RasterCount))\n",
    "    \n",
    "    for i in range(gdal_file.RasterCount):\n",
    "\n",
    "        gdal_file_band=gdal_file.GetRasterBand(i+1)\n",
    "\n",
    "        array[:,:,i]=gdal_file_band.ReadAsArray()\n",
    "        \n",
    "    return(array) \n",
    "\n",
    "#path='D:/Nafiseh/flood_proposal'\n",
    "\n",
    "#path='//tsclient/F/RCM_Abbotsford_Rural/New folder (2)'\n",
    "\n",
    "path='//tsclient/F/QC_2019'\n",
    "\n",
    "#path_test='//tsclient/F/RCM_Abbotsford_Rural/New folder (2)'\n",
    "\n",
    "#GRD_SLC_stack_train_name='Stack_GRD_SLC.tif'\n",
    "\n",
    "#GRD_SLC_stack_test_name='Stack_GRD_SLC_test.tif'\n",
    "\n",
    "#dem_name='dem_gatineau.tif'\n",
    "#dem_name='Abbotsford_dem_2012.tif'\n",
    "#dem_name='DEM.tif'\n",
    "#dem_name='subset_0_of_DEM.tif'\n",
    "#dem_name='dem_leverkhuzen_SRTMGL1.tif'\n",
    "#dem_name_test='subset_1_of_DEM.tif'\n",
    "\n",
    "\n",
    "#train_name='collocate_int_coh_leverkhuzen_2.tif'\n",
    "\n",
    "#train_name='stack_train_RCM_3_20211118_1130.tif'\n",
    "\n",
    "#train_name='subset_0_of_Stack_train_RCM_3_20211118_1130.tif'\n",
    "\n",
    "#train_name='margin_removed_trian.tif'\n",
    "\n",
    "train_name='Stack_GRD_SLC_clip.tif'\n",
    "\n",
    "#train_name='Stack_GRD_SLC.tif'\n",
    "\n",
    "\n",
    "#test_name='collocate_S1B_int_coh.tif'\n",
    "#test_name='stack_test_RCM_2_3_2021119_1130.tif'\n",
    "\n",
    "#test_name='subset_0_of_Stack_test_RCM_2_3_20211119_1130_.tif'\n",
    "#test_name='margin_removed_test.tif'\n",
    "test_name='Stack_GRD_SLC_test_clip.tif'\n",
    "\n",
    "#test_name='Stack_GRD_SLC_test.tif'\n",
    "\n",
    "#gt_name='hydrography_area_mask_leverkhuzen.tif'\n",
    "#gt_name='binary_subset_edge_modified_ground_truth.tif'\n",
    "#gt_name='New Folder/FloodExtentPolygon_CAN_BC_LowerFraser_20211116_020551.tif'\n",
    "#gt_name='New Folder/subset_0_of_FloodExtentPolygon_CAN_BC_LowerFraser_20211116_020551.tif'\n",
    "#gt_name='subset_1_of_FloodExtentPolygon_CAN_BC_LowerFraser_20211116_020551.tif'\n",
    "#gt_name='Subset_2_new_test_of_FloodExtentPolygon_CAN_BC_LowerFraser_20211116_020551.tif'\n",
    "gt_name='gt_test_mask_binary.tif'\n",
    "\n",
    "train_array_int=read_as_array(path,train_name)\n",
    "\n",
    "#temp=train_array_int\n",
    "#temp[temp==0]=10**-10\n",
    "#stack_train_int=np.log10(temp) #vv channel for S1 #HH and HV channels for RCM\n",
    "stack_train_int=train_array_int\n",
    "\n",
    "gt=np.squeeze(read_as_array(path,gt_name),axis=2)\n",
    "#dem=read_as_array(path,dem_name)\n",
    "\n",
    "#dem_test=read_as_array(path_test,dem_name_test)\n",
    "\n",
    "test_array_int=read_as_array(path,test_name)\n",
    "#temp_=test_array_int\n",
    "##temp_=np.copy(test_array)\n",
    "#temp_[temp_==0]=10**-10\n",
    "#stack_test_int=np.log10(temp_) #vv channel for S1 #HH and HV channels for RCM\n",
    "stack_test_int=test_array_int\n",
    "\n",
    "#stack_test_int=zoom(stack_test_int,(gt.shape[0]/stack_test_int.shape[0],gt.shape[1]/stack_test_int.shape[1],1),order=1)\n",
    "\n",
    "row,col=train_array_int.shape[0],train_array_int.shape[1]\n",
    "\n",
    "row_test,col_test=test_array_int.shape[0],test_array_int.shape[1]\n",
    "\n",
    "#dem_row,dem_col=dem.shape[0],dem.shape[1]\n",
    "\n",
    "#dem_row_test,dem_col_test=dem_test.shape[0],dem_test.shape[1]\n",
    "\n",
    "#dem_r=zoom(dem,(row/dem_row,col/dem_col,1),order=0)\n",
    "\n",
    "#dem_r_test=zoom(dem_test,(row_test/dem_row_test,col_test/dem_col_test,1),order=0)\n",
    "#dem_r_test=zoom(dem,(gt.shape[0]/dem_row,gt.shape[1]/dem_col,1),order=0)\n",
    "\n",
    "#GRD_SLC_stack_train_s_db=np.concatenate((stack_train_int[:,:,[0,1,2,3,5,6,8,9]],dem_r),axis=2)\n",
    "\n",
    "#GRD_SLC_stack_train_s=stack_train_int[:,:,[0,1,2,3,5,6,8,9]]\n",
    "\n",
    "#GRD_SLC_stack_train_s=train_array_int[780:1080,0:300,[2,3,9,10,12,13,14,15]]\n",
    "\n",
    "#GRD_SLC_stack_train_s_db=np.concatenate((stack_train_int,dem_r),axis=2)\n",
    "\n",
    "#GRD_SLC_stack_test_s=stack_test_int[:,:,[0,1,2,3,5,6,8,9]]\n",
    "\n",
    "#GRD_SLC_stack_test_s_db=np.concatenate((stack_test_int[:,:,[0,1,2,3,5,6,8,9]],dem_r_test),axis=2)\n",
    "\n",
    "#GRD_SLC_stack_test_s=test_array_int[780:1080,0:300,[2,3,9,10,11,12,13,14]]\n",
    "\n",
    "#GRD_SLC_stack_test_s_db=np.concatenate((stack_test_int,dem_r_test),axis=2)\n",
    "\n",
    "#GRD_SLC_stack_train_s_db=np.concatenate((stack_train_int,dem_r),axis=2)\n",
    "\n",
    "GRD_SLC_stack_train=np.copy(stack_train_int[:,:,[2,3,9,10]])\n",
    "\n",
    "#GRD_SLC_stack_test_s_db=np.concatenate((stack_test_int,dem_r_test),axis=2)\n",
    "\n",
    "GRD_SLC_stack_test=np.copy(stack_test_int[:,:,[2,3,9,10]])\n",
    "\n",
    "GRD_SLC_stack_test[GRD_SLC_stack_test<-10**5]=0\n",
    "\n",
    "\n",
    "#GRD_SLC_stack_train_s=np.concatenate((stack_train_int[780:1080,0:300,[2,3,9,10,12,13,14,15]],dem_r),axis=2)\n",
    "\n",
    "#GRD_SLC_stack_test_s=np.concatenate((stack_test_int[780:1080,0:300,[2,3,9,10,11,12,13,14]],dem_r),axis=2)\n",
    "\n",
    "\n",
    "print(GRD_SLC_stack_train.shape,GRD_SLC_stack_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig,ax=plt.subplots(1,3,figsize=(10,5))\n",
    "\n",
    "ax[0].imshow(GRD_SLC_stack_train[:,:,[0,1,0]])\n",
    "\n",
    "ax[0].axis('off')\n",
    "\n",
    "ax[1].imshow(GRD_SLC_stack_test[:,:,[0,1,0]])\n",
    "\n",
    "ax[1].axis('off')\n",
    "\n",
    "ax[2].imshow(gt)\n",
    "\n",
    "ax[2].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from osgeo import gdal\n",
    "\n",
    "#label_obj=gdal.Open('//tsclient/F/RCM_Abbotsford_Rural/New folder (2)/__subset_2_of_FloodExtentPolygon_CAN_BC_LowerFraser_20211116_020551.tif')\n",
    "\n",
    "label_obj=gdal.Open('//tsclient/F/QC_2019/gt_train_mask_binary.tif')\n",
    "\n",
    "gt_train=label_obj.GetRasterBand(1).ReadAsArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Data Preparation (Part 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_train_data(flood_array,label):\n",
    "\n",
    "    row,col=flood_array.shape[0],flood_array.shape[1]\n",
    "    \n",
    "    neigh_size=33\n",
    "\n",
    "    half_size=int((neigh_size-1)/2)\n",
    "\n",
    "    train_sample=[]\n",
    "\n",
    "    label_new=[]\n",
    "\n",
    "    for i in range(half_size,row-half_size+1):\n",
    "\n",
    "        for j in range(half_size,col-half_size+1):\n",
    "\n",
    "            train_sample.append(flood_array[i-half_size:i+half_size+1,j-half_size:j+half_size+1,:])\n",
    "\n",
    "            #label_new.append(label[i,j])\n",
    "            \n",
    "            label_new.append(label[i-half_size:i+half_size+1,j-half_size:j+half_size+1])\n",
    "\n",
    "            #central_pixel.append((i,j))\n",
    "\n",
    "    #return(train_sample,np.ravel(label_new))\n",
    "    return(train_sample,label_new)\n",
    "\n",
    "def prepare_test_data(postflood_array,label_test):\n",
    "    \n",
    "    neigh_size=33\n",
    "    \n",
    "    half_size=int((neigh_size-1)/2)\n",
    "    \n",
    "    test_sample=[]\n",
    "    \n",
    "    label_new=[]\n",
    "    \n",
    "    postflood_array_pad=np.pad(postflood_array,((half_size,half_size),(half_size,half_size),(0,0)),'symmetric')\n",
    "    \n",
    "    label_test_pad=np.pad(label_test,((half_size,half_size),(half_size,half_size),(0,0)),'symmetric')\n",
    "    \n",
    "    row,col=postflood_array_pad.shape[0],postflood_array_pad.shape[1]\n",
    "    \n",
    "    for i in range(half_size,row-half_size+1):\n",
    "        \n",
    "        for j in range(half_size,col-half_size+1):\n",
    "            \n",
    "            test_sample.append(postflood_array_pad[i-half_size:i+half_size+1,j-half_size:j+half_size+1,:])\n",
    "            \n",
    "            label_new.append(label_test_pad[i-half_size:i+half_size+1,j-half_size:j+half_size+1])\n",
    "            \n",
    "            #label_new.append(label_test_pad[i,j])\n",
    "\n",
    "            \n",
    "    #return(test_sample,np.ravel(label_new))  \n",
    "    return(test_sample,label_new)\n",
    "\n",
    "#train_sample,label=prepare_train_data(intensity_array_vv,label)            \n",
    "#test_sample,label_test=prepare_test_data(postflood_array_,label_test)\n",
    "\n",
    "\n",
    "##label=zoom(label,(300/label.shape[0],300/label.shape[1],1),order=0)\n",
    "gt_train_=np.expand_dims(gt_train,axis=2)\n",
    "train_sample,label=prepare_train_data(zoom(GRD_SLC_stack_train,(gt_train.shape[0]/GRD_SLC_stack_train.shape[0],\\\n",
    "                                                                gt_train.shape[1]/GRD_SLC_stack_train.shape[1],1),order=0),gt_train)\n",
    "                                           \n",
    "#train_sample,label=prepare_train_data(GRD_SLC_stack_train,gt_train_)\n",
    "#mask_intersection_test_r=zoom(mask_intersection_test,[gt.shape[0]/mask_intersection_test.shape[0],gt.shape[1]/mask_intersection_test.shape[1]],order=0)\n",
    "gt=np.expand_dims(gt,axis=2)\n",
    "##label_test=zoom(label_test,(300/label_test.shape[0],300/label_test.shape[1],1),order=0)\n",
    "\n",
    "#GRD_SLC_stack_test_s_db_r=zoom(GRD_SLC_stack_test_s_db,[gt.shape[0]/GRD_SLC_stack_test_s_db.shape[0],gt.shape[1]/GRD_SLC_stack_test_s_db.shape[1],1],order=0)\n",
    "\n",
    "test_sample,label_test=prepare_test_data(zoom(GRD_SLC_stack_test,(gt.shape[0]/GRD_SLC_stack_test.shape[0],gt.shape[1]/GRD_SLC_stack_test.shape[1],1),order=0),gt)\n",
    "                                         \n",
    "#test_sample,label_test=prepare_test_data(GRD_SLC_stack_test,gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig,ax=plt.subplots(1,2,figsize=(10,5))\n",
    "\n",
    "ax[0].imshow(gt_train)\n",
    "\n",
    "ax[0].axis('off')\n",
    "\n",
    "ax[1].imshow(GRD_SLC_stack_train[:,:,[0,1,0]])\n",
    "\n",
    "ax[1].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_sample[0].shape,label[0].shape)\n",
    "\n",
    "print(test_sample[0].shape,label_test[0].shape)\n",
    "\n",
    "print(len(train_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Data Preparation (Part 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train Data Shape: (48600, 32, 32, 4), Train Label Shape: (48600, 32, 32)\n",
      " Test Data Shape: (83583, 32, 32, 4), Test Label Shape: (83583, 32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scipy.ndimage import zoom\n",
    "\n",
    "import random\n",
    "\n",
    "def resample_data(data_sample,label,mode):\n",
    "\n",
    "    data_resampled=[]\n",
    "    \n",
    "    label_new=[]\n",
    "    \n",
    "    patch_size=32\n",
    "    \n",
    "    #if label.ndim==2:\n",
    "       \n",
    "       #label=np.ravel(label) \n",
    "\n",
    "    for k in range(len(data_sample)):\n",
    "        \n",
    "        #print(data_sample[k].shape)\n",
    "        \n",
    "        if data_sample[k].shape<(patch_size,patch_size):\n",
    "\n",
    "            data_resampled.append(zoom(data_sample[k],zoom=[patch_size/data_sample[k].shape[0],patch_size/data_sample[k].shape[1],1],order=0))\n",
    "            \n",
    "            label_new.append(zoom(label[k],zoom=[patch_size/label[k].shape[0],patch_size/label[k].shape[1],1],order=0))\n",
    "\n",
    "            #label_new.append(label[k])\n",
    "            \n",
    "            #continue\n",
    "        \n",
    "        elif data_sample[k].shape==(patch_size,patch_size):\n",
    "            \n",
    "            data_resampled.append(data_sample[k])\n",
    "            \n",
    "            label_new.append(label[k])\n",
    "            \n",
    "        elif data_sample[k].shape>(patch_size,patch_size):\n",
    "            \n",
    "            for i in range(round(data_sample[k].shape[0]/patch_size)):\n",
    "                \n",
    "                for j in range(round(data_sample[k].shape[1]/patch_size)):\n",
    "\n",
    "                \n",
    "                    temp=data_sample[k][i*patch_size:(i+1)*patch_size,j*patch_size:(j+1)*patch_size]\n",
    "                    \n",
    "                    tempp=label[k][i*patch_size:(i+1)*patch_size,j*patch_size:(j+1)*patch_size]\n",
    "                \n",
    "                                \n",
    "                    if temp.shape<(patch_size,patch_size):\n",
    "\n",
    "                    \n",
    "                        data_resampled.append(zoom(temp,zoom=[patch_size/temp.shape[0],patch_size/temp.shape[1],1],order=0))\n",
    "                        \n",
    "                        label_new.append(zoom(tempp,zoom=[patch_size/tempp.shape[0],patch_size/tempp.shape[1],1],order=0))\n",
    "                        \n",
    "                        #continue\n",
    "\n",
    "                        #label_new.append(label[k]) \n",
    "\n",
    "                    else:\n",
    "                    \n",
    "                        data_resampled.append(temp) \n",
    "                        \n",
    "                        label_new.append(tempp)\n",
    "\n",
    "                        #label_new.append(label[k])    \n",
    "                \n",
    "                   \n",
    "\n",
    "    data_resampled=np.stack(data_resampled) \n",
    "\n",
    "    label_new=np.stack(label_new)\n",
    "\n",
    "    print(f' {mode} Data Shape: {data_resampled.shape}, {mode} Label Shape: {label_new.shape}')\n",
    "    \n",
    "    return (np.nan_to_num(data_resampled),np.nan_to_num(label_new))\n",
    "\n",
    "#train_sample_,_,label_,_=train_test_split(train_sample,label,test_size=0.95,stratify=label,random_state=1)\n",
    "\n",
    "train_resampled,label_new=resample_data(train_sample,label,'Train') \n",
    "\n",
    "test_resampled,label_test=resample_data(test_sample,label_test,'Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "\n",
    "def data_augmentation(data,label_new,image_count=150):\n",
    "\n",
    "        augmented_images=[]\n",
    "        \n",
    "        label_aug=[]\n",
    "\n",
    "        datagen_hflip=ImageDataGenerator(rotation_range=0.1, horizontal_flip=True)\n",
    "        \n",
    "        for sn in range(data.shape[0]):\n",
    "\n",
    "            it=datagen_hflip.flow(np.expand_dims(data[sn,:,:,:],axis=0),batch_size=1)\n",
    "\n",
    "            for j in range(image_count):\n",
    "    \n",
    "                augmented_images.append(np.squeeze(it.next(),axis=0))\n",
    "        \n",
    "                label_aug.append(label_new[sn,:,:])\n",
    "        \n",
    "        return(np.stack(augmented_images),np.stack(label_aug)) \n",
    "    \n",
    "#positive_idx,negative_idx=np.where(np.array(label_new)==0),np.where(np.array(label_new)==1)\n",
    "\n",
    "#train_resampled_aug_pos,label_aug_pos=data_augmentation(train_resampled,label_new,positive_idx,20) #positive is background\n",
    "\n",
    "train_resampled_aug,label_aug=data_augmentation(train_resampled,label_new,1)\n",
    "\n",
    "train_resampled_aug=np.concatenate((train_resampled,train_resampled_aug),axis=0)\n",
    "\n",
    "label_aug=np.concatenate((label_new,label_aug),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(data_resampled,mode):\n",
    "    \n",
    "    for i in range(len(data_resampled)):\n",
    "                \n",
    "        for j in range(data_resampled[i].shape[2]):\n",
    "\n",
    "            a=data_resampled[i][:,:,j]\n",
    "\n",
    "            min_,max_=np.min(np.ravel(data_resampled[i][:,:,j])),np.max(np.ravel(data_resampled[i][:,:,j]))\n",
    "\n",
    "            if (min_==0) and (max_==0): \n",
    "                \n",
    "                data_resampled[i][:,:,j]=(a-min_)/(max_-min_+1) \n",
    "            \n",
    "            else:\n",
    "                \n",
    "                data_resampled[i][:,:,j]=(a-min_)/(max_-min_)\n",
    "\n",
    "    print(f' {mode} Data Shape: {data_resampled.shape}') \n",
    "    \n",
    "    return(data_resampled)\n",
    "\n",
    "train_resampled=normalize_data(train_resampled,'Train')\n",
    "\n",
    "test_resampled=normalize_data(test_resampled, 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.save('//tsclient/F/QC_2019/test_resampled_QC_2019.npy',test_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting data into train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train Data Shape: (38880, 32, 32, 4), Validation Data Shape: (9720, 32, 32, 4)\n",
      " \n",
      " Train Label Shape: (38880, 32, 32), Validation Label Shape: (9720, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_resampled,label_new=train_resampled,label_new\n",
    "\n",
    "#positive here is the background\n",
    "\n",
    "train_resampled_,val_resampled,train_label,val_label=train_test_split(train_resampled,label_new,test_size=0.2,random_state=1)\n",
    "\n",
    "print(f' Train Data Shape: {train_resampled_.shape}, Validation Data Shape: {val_resampled.shape}')\n",
    "\n",
    "print(f' \\n Train Label Shape: {train_label.shape}, Validation Label Shape: {val_label.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Train and Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data=np.load('//tsclient/F/QC_2019/train_validation_split.npz')\n",
    "\n",
    "train_resampled_=data['train']\n",
    "train_label=data['train_label']\n",
    "val_resampled=data['val']\n",
    "val_label=data['val_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.savez('//tsclient/F/QC_2019/train_validation_split_Deep-Lab-V3-Plus.npz',train=train_resampled_,train_label=train_label,val=val_resampled,val_label=val_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 4: Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/tensorflow/examples.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "base_model=tf.keras.applications.MobileNetV2(input_shape=[32,32,3],include_top=False)\n",
    "\n",
    "layer_names=['block_1_expand_relu', #size:64\n",
    "            'block_3_expand_relu', #size:32\n",
    "            'block_6_expand_relu', #size:16\n",
    "            'block_13_expand_relu', #size:8\n",
    "            'block_16_expand_relu'] #size:4\n",
    "\n",
    "base_model_outputs=[base_model.get_layer(name).output for name in layer_names]\n",
    "\n",
    "#feature extraction\n",
    "\n",
    "down_stack=tf.keras.Model(inputs=base_model.input,outputs=base_model_outputs)\n",
    "\n",
    "\n",
    "down_stack.trainable=False\n",
    "\n",
    "print(down_stack.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_examples.models.pix2pix import pix2pix\n",
    "\n",
    "up_stack = [\n",
    "    pix2pix.upsample(512, 3),  # 4x4 -> 8x8\n",
    "    pix2pix.upsample(256, 3),  # 8x8 -> 16x16\n",
    "    pix2pix.upsample(128, 3),  # 16x16 -> 32x32\n",
    "    pix2pix.upsample(64, 3),   # 32x32 -> 64x64\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def unet_model(output_channels:int): #output_channels: number of classes\n",
    "    \n",
    "    inputs=tf.keras.layers.Input(shape=[32,32,3])\n",
    "    \n",
    "    skips=down_stack(inputs)\n",
    "    \n",
    "    x=skips[-1]\n",
    "    \n",
    "    skips=reversed(skips[:-1])\n",
    "    \n",
    "    for up,skip in zip(up_stack,skips):\n",
    "        \n",
    "        x=up(x)\n",
    "        \n",
    "        concat=tf.keras.layers.Concatenate()\n",
    "        \n",
    "        #print(x.shape,skip.shape)\n",
    "        \n",
    "        x=concat([x,skip])\n",
    "        \n",
    "        last=tf.keras.layers.Conv2DTranspose(filters=output_channels,kernel_size=3,strides=1,padding='same')\n",
    "        \n",
    "        x=last(x)\n",
    "        \n",
    "        \n",
    "    last_last=tf.keras.layers.Conv2DTranspose(filters=output_channels,kernel_size=3,strides=2,padding='same')  \n",
    "    \n",
    "    x=last_last(x)\n",
    "    print(x.shape)    \n",
    "        \n",
    "    return(tf.keras.Model(inputs=inputs,outputs=x)) \n",
    "\n",
    "num_classes=2\n",
    "\n",
    "embedding=unet_model(output_channels=num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Label arrays Compatible with the Unet Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label[train_label==3]=0\n",
    "\n",
    "val_label[val_label==3]=0\n",
    "\n",
    "print(train_label.shape,val_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_label_back_ground=np.where((train_label==0) | (train_label==1), (train_label.astype(bool))^1,train_label)\n",
    "\n",
    "train_label_unet=np.stack((train_label_back_ground,train_label),axis=3)\n",
    "\n",
    "val_label_back_ground=np.where((val_label==0) | (val_label==1), (val_label.astype(bool))^1,val_label)\n",
    "\n",
    "val_label_unet=np.stack((val_label_back_ground,val_label),axis=3)\n",
    "\n",
    "print(train_label_unet.shape,val_label_unet.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply PCA transfrom on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "def apply_pca_transform(data,n_components=3,patch_size=32):\n",
    "\n",
    "    def make_2d_feature_map_1d(data):\n",
    "\n",
    "        data_ravel=np.zeros((data.shape[0],data.shape[1]*data.shape[2],data.shape[3]))\n",
    "\n",
    "        for i in range(data.shape[0]):\n",
    "\n",
    "            for j in range(data.shape[3]):\n",
    "\n",
    "                 data_ravel[i,:,j]=np.ravel(data[i,:,:,j])\n",
    "\n",
    "        return(data_ravel)      \n",
    "\n",
    "\n",
    "    data_1d=make_2d_feature_map_1d(data)\n",
    "\n",
    "    data_transformed=[]\n",
    "    data_transformed_2d=[]\n",
    "\n",
    "    pca=PCA(n_components=3)\n",
    "\n",
    "    for k in range(data_1d.shape[0]):\n",
    "\n",
    "        data_transformed.append(pca.fit_transform(data_1d[k,:,:]))\n",
    "\n",
    "        data_transformed_2d.append(np.reshape(data_transformed[k],(patch_size,patch_size,n_components)))\n",
    "      \n",
    "    return(np.array(data_transformed_2d))\n",
    "\n",
    "train_resampled__=apply_pca_transform(np.nan_to_num(train_resampled_))\n",
    "val_resampled__=apply_pca_transform(np.nan_to_num(val_resampled))\n",
    "\n",
    "print(train_resampled__.shape,val_resampled__.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting Unet Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "\n",
    "\n",
    "batch_size,epochs=1,200\n",
    "\n",
    "lr_schedule=ExponentialDecay(0.0001,decay_rate=0.8,decay_steps=10000)\n",
    "\n",
    "embedding.compile(loss='binary_crossentropy',optimizer=Adam(learning_rate=lr_schedule),metrics=['accuracy'])\n",
    "\n",
    "es=EarlyStopping(monitor='val_loss',patience=30,mode='min',restore_best_weights=True)\n",
    "\n",
    "mch=ModelCheckpoint(filepath='D:/Nafiseh/flood_proposal/Abbotsford_suburbun_new_train_data_unet_model.h5',save_best_only=True)\n",
    "\n",
    "history=embedding.fit(train_resampled__,train_label,batch_size=1,epochs=epochs,callbacks=[es,mch],validation_data=(val_resampled__,val_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "\n",
    "plt.plot(history.history['val_loss'])\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.ylabel('Binary Cross Entropy Loss')\n",
    "\n",
    "plt.legend(['Train','Validation'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.legend(['Train','Validation'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding.save('D:/Nafiseh/flood_proposal/unet_Abbotsford_suburban_new_train_data.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "embedding=load_model('D:/Nafiseh/flood_proposal/QC_2019_new_train_data_unet_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('D:/Nafiseh/flood_proposal/training_history_unet_Abbotsford_suburbun_new_train_data.pckl', 'wb') as hist:\n",
    "    \n",
    "    pickle.dump(history.history,hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_resampled__=apply_pca_transform(test_resampled)\n",
    "\n",
    "patch_size=32\n",
    "\n",
    "predicted_segments=np.argmax(embedding.predict(test_resampled__),axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig,axes=plt.subplots(4,1,figsize=(20,10))\n",
    "\n",
    "ax=np.ravel(axes)\n",
    "\n",
    "for p in range(4):\n",
    "\n",
    "    patch=ax[p].imshow(predicted_segments[p,:,:])\n",
    "\n",
    "    ax[p].axis('off')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.colorbar(patch,ax=ax.tolist(),ticks=[0,1])    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "#map_r,map_c=992,2038\n",
    "\n",
    "#map_r,map_c=263, 615\n",
    "\n",
    "#map_r,map_c=300,300\n",
    "\n",
    "#map_r,map_c=101,101\n",
    "\n",
    "map_r,map_c=gt.shape[0],gt.shape[1]\n",
    "\n",
    "#patch_size=128\n",
    "\n",
    "patch_size=32\n",
    "\n",
    "num_r,num_c=int(map_r/patch_size),int(map_c/patch_size)\n",
    "\n",
    "predicted_segments=np.load('//tsclient/F/QC_2019/predicted_segments_QC_2019_unet.npy')\n",
    "\n",
    "print(predicted_segments.shape)\n",
    "\n",
    "for ii in range(num_r):\n",
    "    \n",
    "    if ii==0:\n",
    "\n",
    "        initial=np.hstack(predicted_segments[ii*num_c:ii*num_c+num_c,:,:])\n",
    "    else:\n",
    "        \n",
    "        initial=np.vstack((initial,np.hstack(predicted_segments[ii*num_c:ii*num_c+num_c,:,:]))) \n",
    "        \n",
    "unet_map=np.copy(initial)\n",
    "        \n",
    "print(unet_map.shape,np.unique(unet_map))    \n",
    "\n",
    "plt.imshow(unet_map)\n",
    "\n",
    "plt.axis('off')\n",
    "    \n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.save('D:/Nafiseh/flood_proposal/unet_map_Abbotsford_suburbun_new_train_data.npy',unet_map)\n",
    "\n",
    "#unet_map=np.load('D:/Nafiseh/flood_proposal/unet_map_Abbotsford_with_dem.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from osgeo import gdal,osr\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import utm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#path='D:/Nafiseh/flood_proposal'\n",
    "\n",
    "path='//tsclient/F/RCM_Abbotsford_Rural/'\n",
    "\n",
    "#stack_image_name='subset_0_of_S1A_IW_GRDH_1SDV_Cal_Spk_TC_Stack.tif'\n",
    "\n",
    "#stack_image_name='subset_1_of_S1A_IW_GRDH_1SDV_20190408T225218_20190507T23009_Cal_Spk_TC_Stack_2_subset.tif'\n",
    "\n",
    "#stack_image_name='collocate_S1B_int_coh.tif'\n",
    "\n",
    "#stack_image_name='stack_train_RCM_3_20211118_1130.tif'\n",
    "\n",
    "#stack_image_name='New folder (2)/subset_1_of_Stack_test_RCM_2_3_20211119_1130.tif'\n",
    "\n",
    "stack_image_name='New folder (2)/margin_removed_test_reporject.tif'\n",
    "\n",
    "#Rapid_Eye_Image_name='clip_0428_clip_3.tif'\n",
    "\n",
    "#gt_name='hydrography_area_mask_leverkhuzen_clip.tif'\n",
    "\n",
    "#gt_name='binary_subset_edge_modified_ground_truth.tif'\n",
    "\n",
    "gt_name='New folder (2)/Subset_2_new_test_of_FloodExtentPolygon_CAN_BC_LowerFraser_20211116_020551.tif'\n",
    "\n",
    "#gt=read_as_array(path,gt_name)\n",
    "\n",
    "#image_array=read_as_array(path,Rapid_Eye_Image_name)\n",
    "\n",
    "ds=gdal.Open(path+'/'+stack_image_name)\n",
    "\n",
    "#rapid_eye_ds=gdal.Open(path+'/'+Rapid_Eye_Image_name)\n",
    "\n",
    "ulx,xres,xskew,uly,yskew,yres=ds.GetGeoTransform()\n",
    "\n",
    "#uly_eye,yres_eye,yskew_eye,ulx_eye,xskew_eye,xres_eye=rapid_eye_ds.GetGeoTransform()\n",
    "\n",
    "#row,col=1501,1501\n",
    "\n",
    "#row,col=1379-1011+1,1055-505+1\n",
    "\n",
    "#row,col=301,301\n",
    "\n",
    "#row,col=992, 2038\n",
    "\n",
    "#row,col=263, 615\n",
    "\n",
    "row,col=96,96\n",
    "\n",
    "#row_ul_new,col_ul_new=780,0\n",
    "row_ul_new,col_ul_new=0, 0\n",
    "\n",
    "#row_lr_new,col_lr_new=1080,300\n",
    "#row_lr_new,col_lr_new=992, 2038\n",
    "#row_lr_new,col_lr_new=263, 615\n",
    "\n",
    "\n",
    "gt=read_as_array(path,gt_name)\n",
    "\n",
    "\n",
    "#gt=mask_union_test\n",
    "\n",
    "change_map=np.copy(unet_map)\n",
    "\n",
    "\n",
    "def find_new_pixel_cor(row_new,col_new,ulx,uly,ulx_eye,uly_eye,xres,yres,xres_eye,yres_eye):\n",
    "\n",
    "    x_new=col_new*xres+ulx\n",
    "\n",
    "    y_new=row_new*yres+uly\n",
    "\n",
    "    utm_cor=utm.from_latlon(y_new,x_new)\n",
    "\n",
    "    y_new_utm,x_new_utm=utm_cor[0],utm_cor[1]\n",
    "\n",
    "    row_eye=np.int(np.floor((y_new_utm-uly_eye)/yres_eye))\n",
    "\n",
    "    col_eye=np.int(np.floor((x_new_utm-ulx_eye)/xres_eye))\n",
    "    \n",
    "    return(x_new,y_new,row_eye,col_eye)\n",
    "\n",
    "#x_new,y_new,row_ul_eye,col_ul_eye=find_new_pixel_cor(row_ul_new,col_ul_new,ulx,uly,ulx_eye,uly_eye,xres,yres,xres_eye,yres_eye)\n",
    "\n",
    "#_,_,row_lr_eye,col_lr_eye=find_new_pixel_cor(row_lr_new,col_lr_new,ulx,uly,ulx_eye,uly_eye,xres,yres,xres_eye,yres_eye)\n",
    "\n",
    "#RGB_image_array=image_array[:row_lr_eye+1,col_ul_eye:col_lr_eye+1,[2,1,0]]\n",
    "\n",
    "#ul_x_new,ul_y_new=x_new,y_new\n",
    "\n",
    "#BBOX=[ul_x_new, ul_x_new+xres*300, ul_y_new+yres*300, ul_y_new]\n",
    "#BBOX=[6.9, 6.9+xres*2038, 50.983+yres*992, 51.072]\n",
    "#BBOX=[-122.237, -122.226, 49.039, 49.044]\n",
    "\n",
    "#BBOX=[-122.27, -122.27+xres*96, 49.04+yres*96, 49.04]\n",
    "\n",
    "BBOX=[-122.2346,-122.2125,49.005,49.019]\n",
    "\n",
    "\n",
    "#change_map=np.reshape(test_label_p,(row,col))\n",
    "\n",
    "orig_map=plt.cm.get_cmap('binary')\n",
    "\n",
    "reversed_map=orig_map.reversed()\n",
    "\n",
    "fig,ax=plt.subplots(1,2,figsize=(10,5))\n",
    "\n",
    "#plt.setp(ax, xticks=np.round(np.arange(BBOX[0],BBOX[1],0.05),2), yticks=np.round(np.arange(BBOX[2],BBOX[3],0.05),3))\n",
    "\n",
    "\n",
    "#just for unet\n",
    "#change_map=1-change_map\n",
    "\n",
    "ax[0].set_xticks(ticks=np.round(np.arange(BBOX[0],BBOX[1],0.01),4),labels=np.round(np.arange(BBOX[0],BBOX[1],0.01),4),fontsize=12)\n",
    "\n",
    "ax[0].set_yticks(ticks=np.round(np.arange(BBOX[2],BBOX[3],0.005),4),labels=np.round(np.arange(BBOX[2],BBOX[3],0.005),4),fontsize=12)\n",
    "\n",
    "ax[0].imshow(change_map, extent=BBOX, aspect='equal',cmap=reversed_map)\n",
    "\n",
    "#ax[0].set_title('Flood Map \\n 07/05/2019',fontname='Times New Roman', fontweight='bold', fontsize=14)\n",
    "#ax[0].set_title('Change Map \\n 18/07/2021',fontname='Times New Roman', fontweight='bold', fontsize=14)\n",
    "ax[0].set_title('Flood Map',fontname='Times New Roman', fontweight='bold', fontsize=12)\n",
    "\n",
    "\n",
    "ax[1].set_xticks(ticks=np.round(np.arange(BBOX[0],BBOX[1],0.01),4),labels=np.round(np.arange(BBOX[0],BBOX[1],0.01),4),fontsize=12)\n",
    "\n",
    "ax[1].set_yticks(ticks=np.round(np.arange(BBOX[2],BBOX[3],0.005),4),labels=np.round(np.arange(BBOX[2],BBOX[3],0.005),4),fontsize=12)\n",
    "\n",
    "ax[1].imshow(gt,extent=BBOX, aspect='equal', cmap=reversed_map)\n",
    "\n",
    "ax[1].set_title('Ground Truth Mask',fontname='Times New Roman', fontweight='bold',fontsize=12)\n",
    "\n",
    "#ax[2].imshow(np.divide(RGB_image_array,(8,8,8)).astype('uint8'), extent=BBOX)\n",
    "\n",
    "#ax[2].set_title('Rapid Eye RGB image \\n 28/04/2019', fontname='Times New Roman', fontweight='bold',fontsize=14)\n",
    "\n",
    "fig.tight_layout(pad=3)\n",
    "\n",
    "#plt.savefig('D:/Nafiseh/flood_proposal/intensity_unet_with_dem_Abbotsford_suburbun.tif')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "\n",
    "#gt=mask_union_test\n",
    "\n",
    "gt[gt==3]=0\n",
    "\n",
    "#gt[gt==3]=1\n",
    "\n",
    "change_map=unet_map\n",
    "\n",
    "change_map_r=zoom(change_map,[gt.shape[0]/change_map.shape[0],gt.shape[1]/change_map.shape[1]],order=1)\n",
    "\n",
    "sns.heatmap(confusion_matrix(np.ravel(gt),np.ravel(change_map_r)),annot=True,fmt='d')\n",
    "\n",
    "plt.savefig('D:/Nafiseh/flood_proposal/confusion_matrix_unet_QC_2019_new_train_data.tif')\n",
    "\n",
    "text_file=open('D:/Nafiseh/flood_proposal/classification_report_unet_QC_2019_new_train_data.txt','w')\n",
    "\n",
    "print(classification_report(np.ravel(gt),np.ravel(change_map_r)),file=text_file)\n",
    "\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from osgeo import gdal, osr\n",
    "\n",
    "def Write_array_as_geotiff(array,ul_x_new,ul_y_new,raster_name):\n",
    "\n",
    "    #ds=gdal.Open('D:/Nafiseh/flood_proposal/collocate_S1B_int_coh.tif')\n",
    "    \n",
    "    ds=gdal.Open('//tsclient/F/RCM_Abbotsford_Rural/New folder (2)/subset_1_of_Stack_test_RCM_2_3_20211119_1130.tif')\n",
    "\n",
    "    ulx,xres,xskew,uly,yskew,yres=ds.GetGeoTransform()\n",
    "\n",
    "    geotransform=(ul_x_new, xres, xskew, ul_y_new, yskew, yres)\n",
    "\n",
    "    raster=gdal.GetDriverByName('GTiff').Create(raster_name,96,96,1,gdal.GDT_Float32)\n",
    "\n",
    "    raster.SetGeoTransform(geotransform)\n",
    "\n",
    "    srs=osr.SpatialReference()\n",
    "\n",
    "    srs.ImportFromEPSG(4326)\n",
    "    \n",
    "    raster.GetRasterBand(1).WriteArray(array)\n",
    "    \n",
    "    raster.GetRasterBand(1).SetNoDataValue(0)\n",
    "    \n",
    "    raster.FlushCache() ##saves to disk\n",
    "    \n",
    "    raster=None\n",
    "    \n",
    "    ds=None\n",
    "\n",
    "\n",
    "Write_array_as_geotiff(unet_plus_2d_map,0,0,'D:/Nafiseh/flood_proposal/CHM_unet_plus_2d_Abbotsford_suburbun.tif')\n",
    "\n",
    "#Write_array_as_geotiff(label_test_new_2D,ul_x_new,ul_y_new,'D:/Nafiseh/flood_proposal/Change_mask_07052019.tif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Change Map and Ground Truth Map as Geo Tiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from osgeo import gdal, osr\n",
    "\n",
    "def Write_array_as_geotiff(array,ul_x_new,ul_y_new,raster_name):\n",
    "\n",
    "    ds=gdal.Open('D:/Nafiseh/flood_proposal/subset_0_of_S1A_IW_GRDH_1SDV_Cal_Spk_TC_Stack.tif')\n",
    "\n",
    "    ulx,xres,xskew,uly,yskew,yres=ds.GetGeoTransform()\n",
    "\n",
    "    geotransform=(ul_x_new, xres, xskew, ul_y_new, yskew, yres)\n",
    "\n",
    "    raster=gdal.GetDriverByName('GTiff').Create(raster_name,301,301,1,gdal.GDT_Float32)\n",
    "\n",
    "    raster.SetGeoTransform(geotransform)\n",
    "\n",
    "    srs=osr.SpatialReference()\n",
    "\n",
    "    srs.ImportFromEPSG(4326)\n",
    "    \n",
    "    raster.GetRasterBand(1).WriteArray(array)\n",
    "    \n",
    "    raster.GetRasterBand(1).SetNoDataValue(0)\n",
    "    \n",
    "    raster.FlushCache() ##saves to disk\n",
    "    \n",
    "    raster=None\n",
    "    \n",
    "    ds=None\n",
    "\n",
    "\n",
    "Write_array_as_geotiff(change_map,ul_x_new,ul_y_new,'D:/Nafiseh/flood_proposal/CHM_contrastive_Loss_downsampling_more_train_data.tif')\n",
    "\n",
    "#Write_array_as_geotiff(label_test_new_2D,ul_x_new,ul_y_new,'D:/Nafiseh/flood_proposal/Change_mask_07052019.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.layers import Input, Conv2D, ZeroPadding2D, UpSampling2D, Dense, concatenate, Conv2DTranspose\n",
    "from tensorflow.keras.layers import MaxPooling2D, GlobalAveragePooling2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout, Flatten, Lambda\n",
    "from tensorflow.keras.layers import ELU, LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import GaussianDropout\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "smooth = 1.\n",
    "dropout_rate = 0.5\n",
    "\n",
    "def mean_iou(y_true, y_pred):\n",
    "    prec = []\n",
    "    for t in np.arange(0.5, 1.0, 0.05):\n",
    "        y_pred_ = tf.to_int32(y_pred > t)\n",
    "        score, up_opt = tf.metrics.mean_iou(y_true, y_pred_, 2)\n",
    "        K.get_session().run(tf.local_variables_initializer())\n",
    "        with tf.control_dependencies([up_opt]):\n",
    "            score = tf.identity(score)\n",
    "        prec.append(score)\n",
    "    return K.mean(K.stack(prec), axis=0)\n",
    "\n",
    "# Custom loss function\n",
    "def dice_coef(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    return 0.5 * keras.losses.binary_crossentropy(y_true, y_pred) - dice_coef(y_true, y_pred)\n",
    "\n",
    "\n",
    "########################################\n",
    "# 2D Standard\n",
    "########################################\n",
    "\n",
    "def standard_unit(input_tensor, stage, nb_filter, kernel_size=3):\n",
    "\n",
    "    act = 'elu'\n",
    "\n",
    "    x = Conv2D(nb_filter, (kernel_size, kernel_size), activation=act, name='conv'+stage+'_1', kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4))(input_tensor)\n",
    "    x = Dropout(dropout_rate, name='dp'+stage+'_1')(x)\n",
    "    x = Conv2D(nb_filter, (kernel_size, kernel_size), activation=act, name='conv'+stage+'_2', kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4))(x)\n",
    "    x = Dropout(dropout_rate, name='dp'+stage+'_2')(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Standard UNet++ [Zhou et.al, 2018]\n",
    "Total params: 9,041,601\n",
    "\"\"\"\n",
    "def Nest_Net(img_rows, img_cols, color_type=1, num_class=1, deep_supervision=False):\n",
    "\n",
    "    nb_filter = [32,64,128,256,512]\n",
    "    act = 'elu'\n",
    "\n",
    "    # Handle Dimension Ordering for different backends\n",
    "    global bn_axis\n",
    "    #if K.image_data_format() == 'tf':\n",
    "      #bn_axis = 3\n",
    "    img_input = Input(shape=(img_rows, img_cols, color_type), name='main_input')\n",
    "    #else:\n",
    "      #bn_axis = 1\n",
    "      #img_input = Input(shape=(color_type, img_rows, img_cols), name='main_input')\n",
    "\n",
    "    conv1_1 = standard_unit(img_input, stage='11', nb_filter=nb_filter[0])\n",
    "    \n",
    "    print(conv1_1.shape)\n",
    "    pool1 = MaxPooling2D((2, 2), strides=(2, 2), name='pool1')(conv1_1)\n",
    "\n",
    "    conv2_1 = standard_unit(pool1, stage='21', nb_filter=nb_filter[1])\n",
    "    pool2 = MaxPooling2D((2, 2), strides=(2, 2), name='pool2')(conv2_1)\n",
    "\n",
    "    up1_2 = Conv2DTranspose(nb_filter[0], (2, 2), strides=(2, 2), name='up12', padding='same')(conv2_1)\n",
    "    conv1_2 = concatenate([up1_2, conv1_1], name='merge12', axis=bn_axis)\n",
    "    conv1_2 = standard_unit(conv1_2, stage='12', nb_filter=nb_filter[0])\n",
    "\n",
    "    conv3_1 = standard_unit(pool2, stage='31', nb_filter=nb_filter[2])\n",
    "    pool3 = MaxPooling2D((2, 2), strides=(2, 2), name='pool3')(conv3_1)\n",
    "\n",
    "    up2_2 = Conv2DTranspose(nb_filter[1], (2, 2), strides=(2, 2), name='up22', padding='same')(conv3_1)\n",
    "    conv2_2 = concatenate([up2_2, conv2_1], name='merge22', axis=bn_axis)\n",
    "    conv2_2 = standard_unit(conv2_2, stage='22', nb_filter=nb_filter[1])\n",
    "    \n",
    "    print(conv2_2.shape)\n",
    "\n",
    "    up1_3 = Conv2DTranspose(nb_filter[0], (2, 2), strides=(2, 2), name='up13', padding='same')(conv2_2)\n",
    "    conv1_3 = concatenate([up1_3, conv1_1, conv1_2], name='merge13', axis=bn_axis)\n",
    "    conv1_3 = standard_unit(conv1_3, stage='13', nb_filter=nb_filter[0])\n",
    "\n",
    "    conv4_1 = standard_unit(pool3, stage='41', nb_filter=nb_filter[3])\n",
    "    pool4 = MaxPooling2D((2, 2), strides=(2, 2), name='pool4')(conv4_1)\n",
    "\n",
    "    up3_2 = Conv2DTranspose(nb_filter[2], (2, 2), strides=(2, 2), name='up32', padding='same')(conv4_1)\n",
    "    conv3_2 = concatenate([up3_2, conv3_1], name='merge32', axis=bn_axis)\n",
    "    conv3_2 = standard_unit(conv3_2, stage='32', nb_filter=nb_filter[2])\n",
    "\n",
    "    up2_3 = Conv2DTranspose(nb_filter[1], (2, 2), strides=(2, 2), name='up23', padding='same')(conv3_2)\n",
    "    conv2_3 = concatenate([up2_3, conv2_1, conv2_2], name='merge23', axis=bn_axis)\n",
    "    conv2_3 = standard_unit(conv2_3, stage='23', nb_filter=nb_filter[1])\n",
    "\n",
    "    up1_4 = Conv2DTranspose(nb_filter[0], (2, 2), strides=(2, 2), name='up14', padding='same')(conv2_3)\n",
    "    conv1_4 = concatenate([up1_4, conv1_1, conv1_2, conv1_3], name='merge14', axis=bn_axis)\n",
    "    conv1_4 = standard_unit(conv1_4, stage='14', nb_filter=nb_filter[0])\n",
    "\n",
    "    conv5_1 = standard_unit(pool4, stage='51', nb_filter=nb_filter[4])\n",
    "\n",
    "    up4_2 = Conv2DTranspose(nb_filter[3], (2, 2), strides=(2, 2), name='up42', padding='same')(conv5_1)\n",
    "    conv4_2 = concatenate([up4_2, conv4_1], name='merge42', axis=bn_axis)\n",
    "    conv4_2 = standard_unit(conv4_2, stage='42', nb_filter=nb_filter[3])\n",
    "\n",
    "    up3_3 = Conv2DTranspose(nb_filter[2], (2, 2), strides=(2, 2), name='up33', padding='same')(conv4_2)\n",
    "    conv3_3 = concatenate([up3_3, conv3_1, conv3_2], name='merge33', axis=bn_axis)\n",
    "    conv3_3 = standard_unit(conv3_3, stage='33', nb_filter=nb_filter[2])\n",
    "\n",
    "    up2_4 = Conv2DTranspose(nb_filter[1], (2, 2), strides=(2, 2), name='up24', padding='same')(conv3_3)\n",
    "    conv2_4 = concatenate([up2_4, conv2_1, conv2_2, conv2_3], name='merge24', axis=bn_axis)\n",
    "    conv2_4 = standard_unit(conv2_4, stage='24', nb_filter=nb_filter[1])\n",
    "\n",
    "    up1_5 = Conv2DTranspose(nb_filter[0], (2, 2), strides=(2, 2), name='up15', padding='same')(conv2_4)\n",
    "    conv1_5 = concatenate([up1_5, conv1_1, conv1_2, conv1_3, conv1_4], name='merge15', axis=bn_axis)\n",
    "    conv1_5 = standard_unit(conv1_5, stage='15', nb_filter=nb_filter[0])\n",
    "\n",
    "    nestnet_output_1 = Conv2D(num_class, (1, 1), activation='sigmoid', name='output_1', kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4))(conv1_2)\n",
    "    nestnet_output_2 = Conv2D(num_class, (1, 1), activation='sigmoid', name='output_2', kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4))(conv1_3)\n",
    "    nestnet_output_3 = Conv2D(num_class, (1, 1), activation='sigmoid', name='output_3', kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4))(conv1_4)\n",
    "    nestnet_output_4 = Conv2D(num_class, (1, 1), activation='sigmoid', name='output_4', kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4))(conv1_5)\n",
    "\n",
    "    if deep_supervision:\n",
    "        model = Model(inputs=img_input, outputs=[nestnet_output_1,\n",
    "                                               nestnet_output_2,\n",
    "                                               nestnet_output_3,\n",
    "                                               nestnet_output_4])\n",
    "    else:\n",
    "        model = Model(inputs=img_input, outputs=[nestnet_output_4])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    model = Nest_Net(32,32,3)\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras-unet-collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_unet_collection import models\n",
    "\n",
    "unet_plus_2d = models.unet_plus_2d((32, 32, 3), [64, 128, 256, 512], n_labels=2,\n",
    "                            stack_num_down=2, stack_num_up=2,\n",
    "                            activation='LeakyReLU', output_activation='Softmax', \n",
    "                            batch_norm=False, pool='max', unpool=False, deep_supervision=True, name='xnet')\n",
    "\n",
    "unet_plus_2d.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "\n",
    "batch_size,epochs=1, 200\n",
    "\n",
    "lr_schedule=ExponentialDecay(0.0001,decay_rate=0.8,decay_steps=10000)\n",
    "\n",
    "unet_plus_2d.compile(loss='binary_crossentropy',optimizer=Adam(learning_rate=lr_schedule),metrics=['accuracy'])\n",
    "\n",
    "es=EarlyStopping(monitor='val_loss',patience=50,restore_best_weights=True)\n",
    "\n",
    "mch=ModelCheckpoint(filepath='D:/Nafiseh/flood_proposal/QC_2019_new_train_data_Unet_plus_2d.h5',save_best_only=True)\n",
    "\n",
    "history_unet_plus_2d=unet_plus_2d.fit(train_resampled__,train_label_unet,batch_size,epochs=epochs,callbacks=[es,mch],validation_data=(val_resampled__,val_label_unet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "plt.plot(history_unet_plus_2d.history['loss'])\n",
    "\n",
    "plt.plot(history_unet_plus_2d.history['val_loss'])\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.ylabel('Binary Cross Entropy')\n",
    "\n",
    "plt.legend(['Train','Validation'])\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "plt.plot(history_unet_plus_2d.history['xnet_output_final_activation_accuracy'])\n",
    "\n",
    "plt.plot(history_unet_plus_2d.history['val_xnet_output_final_activation_accuracy'])\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.legend(['Train','Validation'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(unet_plus_2d.metrics_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_plus_2d.save('D:/Nafiseh/flood_proposal/unet_plus_2d_Abbotsford_suburbun_new_train_data.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('D:/Nafiseh/flood_proposal/training_history_unet_plus_2d_Abbotsford_suburbun_new_train_data.pckl', 'wb') as hist:\n",
    "    \n",
    "    pickle.dump(history_unet_plus_2d.history,hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_resampled__=apply_pca_transform(test_resampled)\n",
    "\n",
    "patch_size=32\n",
    "\n",
    "predicted_segments=np.argmax(unet_plus_2d.predict(test_resampled__)[-1],axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "\n",
    "#map_r,map_c=992,2038\n",
    "\n",
    "#map_r,map_c=263, 615\n",
    "\n",
    "map_r,map_c=gt.shape[0],gt.shape[1]\n",
    "\n",
    "patch_size=13\n",
    "\n",
    "num_r,num_c=int(map_r/patch_size),int(map_c/patch_size)\n",
    "\n",
    "print(predicted_segments.shape)\n",
    "\n",
    "for ii in range(num_r):\n",
    "    \n",
    "    if ii==0:\n",
    "\n",
    "        initial=np.hstack(predicted_segments[ii*num_c:ii*num_c+num_c,:,:])\n",
    "    else:\n",
    "        \n",
    "        initial=np.vstack((initial,np.hstack(predicted_segments[ii*num_c:ii*num_c+num_c,:,:]))) \n",
    "        \n",
    "unet_plus_2d_map=np.copy(initial)\n",
    "        \n",
    "print(unet_plus_2d_map.shape)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.imshow(unet_plus_2d_map)\n",
    "\n",
    "plt.axis('off')\n",
    "    \n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('D:/Nafiseh/flood_proposal/unet_plus_2d_map_Abbotsford_suburbun.npy',unet_plus_2d_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "\n",
    "#gt=mask_union_test\n",
    "\n",
    "gt[gt==2]=1\n",
    "\n",
    "#gt[gt==3]=1\n",
    "\n",
    "change_map=unet_plus_2d_map\n",
    "\n",
    "change_map_r=zoom(change_map,[gt.shape[0]/change_map.shape[0],gt.shape[1]/change_map.shape[1]],order=0)\n",
    "\n",
    "sns.heatmap(confusion_matrix(np.ravel(gt),np.ravel(change_map_r)),annot=True,fmt='d')\n",
    "\n",
    "plt.savefig('D:/Nafiseh/flood_proposal/confusion_matrix_unet_plus_2d_Abbotsford_suburbun_new_train_data.tif')\n",
    "\n",
    "text_file=open('D:/Nafiseh/flood_proposal/classification_report_unet_plus_2d_Abbotsford_suburbun_new_train_data.txt','w')\n",
    "\n",
    "print(classification_report(np.ravel(gt),np.ravel(change_map_r)),file=text_file)\n",
    "\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepLabV3-Plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, UpSampling2D\n",
    "from tensorflow.keras.layers import AveragePooling2D, Conv2DTranspose, Concatenate, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "\n",
    "\n",
    "# Custom loss function\n",
    "def dice_coef(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "\"\"\" Atrous Spatial Pyramid Pooling \"\"\"\n",
    "def ASPP(inputs):\n",
    "    shape = inputs.shape\n",
    "\n",
    "    y_pool = AveragePooling2D(pool_size=(shape[1], shape[2]), name='average_pooling')(inputs)\n",
    "    y_pool = Conv2D(filters=64, kernel_size=1, padding='same', use_bias=False)(y_pool)\n",
    "    y_pool = BatchNormalization(name=f'bn_1')(y_pool)\n",
    "    y_pool = Activation('relu', name=f'relu_1')(y_pool)\n",
    "    y_pool = UpSampling2D((shape[1], shape[2]), interpolation=\"bilinear\")(y_pool)\n",
    "\n",
    "    y_1 = Conv2D(filters=64, kernel_size=1, dilation_rate=1, padding='same', use_bias=False)(inputs)\n",
    "    y_1 = BatchNormalization()(y_1)\n",
    "    y_1 = Activation('relu')(y_1)\n",
    "\n",
    "    y_6 = Conv2D(filters=64, kernel_size=3, dilation_rate=6, padding='same', use_bias=False)(inputs)\n",
    "    y_6 = BatchNormalization()(y_6)\n",
    "    y_6 = Activation('relu')(y_6)\n",
    "\n",
    "    y_12 = Conv2D(filters=64, kernel_size=3, dilation_rate=12, padding='same', use_bias=False)(inputs)\n",
    "    y_12 = BatchNormalization()(y_12)\n",
    "    y_12 = Activation('relu')(y_12)\n",
    "\n",
    "    y_18 = Conv2D(filters=64, kernel_size=3, dilation_rate=18, padding='same', use_bias=False)(inputs)\n",
    "    y_18 = BatchNormalization()(y_18)\n",
    "    y_18 = Activation('relu')(y_18)\n",
    "\n",
    "    y = Concatenate()([y_pool, y_1, y_6, y_12, y_18])\n",
    "\n",
    "    y = Conv2D(filters=64, kernel_size=1, dilation_rate=1, padding='same', use_bias=False)(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "    return y\n",
    "\n",
    "def DeepLabV3Plus(shape):\n",
    "    \"\"\" Inputs \"\"\"\n",
    "    inputs = Input(shape)\n",
    "\n",
    "    \"\"\" Pre-trained ResNet50 \"\"\"\n",
    "    base_model = ResNet50(weights=None, include_top=False, input_tensor=inputs)\n",
    "\n",
    "    \"\"\" Pre-trained ResNet50 Output \"\"\"\n",
    "    image_features = base_model.get_layer('conv4_block6_out').output\n",
    "    x_a = ASPP(image_features)\n",
    "    x_a = UpSampling2D((4, 4), interpolation=\"bilinear\")(x_a)\n",
    "\n",
    "    \"\"\" Get low-level features \"\"\"\n",
    "    x_b = base_model.get_layer('conv2_block2_out').output\n",
    "    x_b = Conv2D(filters=48, kernel_size=1, padding='same', use_bias=False)(x_b)\n",
    "    x_b = BatchNormalization()(x_b)\n",
    "    x_b = Activation('relu')(x_b)\n",
    "\n",
    "    x = Concatenate()([x_a, x_b]) \n",
    "\n",
    "    x = Conv2D(filters=256, kernel_size=3, padding='same', activation='relu',use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters=256, kernel_size=3, padding='same', activation='relu', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = UpSampling2D((4, 4), interpolation=\"bilinear\")(x)\n",
    "\n",
    "    \"\"\" Outputs \"\"\"\n",
    "    x = Conv2D(1, (1, 1), name='output_layer')(x)\n",
    "    x = Activation('sigmoid')(x)\n",
    "    \n",
    "    \"\"\" Model \"\"\"\n",
    "    model = Model(inputs=inputs, outputs=x)\n",
    "    return model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_shape = (32, 32, 3)\n",
    "    model_DeepLabV3Plus = DeepLabV3Plus(input_shape)\n",
    "    model_DeepLabV3Plus.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "\n",
    "batch_size,epochs=20, 200\n",
    "\n",
    "lr_schedule=ExponentialDecay(0.0001,decay_rate=0.8,decay_steps=10000)\n",
    "\n",
    "model_DeepLabV3Plus.compile(loss='binary_crossentropy',optimizer=Adam(learning_rate=lr_schedule),metrics=['accuracy'])\n",
    "\n",
    "es=EarlyStopping(monitor='val_loss',patience=30,restore_best_weights=True)\n",
    "\n",
    "mch=ModelCheckpoint(filepath='D:/Nafiseh/flood_proposal/Abbotsford_suburban_new_train_data_DeepLabV3_Plus.h5',save_best_only=True)\n",
    "\n",
    "history_DeepLabV3Plus=model_DeepLabV3Plus.fit(train_resampled__,train_label,batch_size,epochs=epochs,callbacks=[es,mch],validation_data=(val_resampled__,val_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.min(val_resampled__),np.max(val_resampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_DeepLabV3Plus.save('D:/Nafiseh/flood_proposal/DeepLabV3_Plus_RCM_Abbotsford_suburban_new_train_data.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('D:/Nafiseh/flood_proposal/training_history_deep_lab_v3_plus_RCM_Abbotsford_suburban_new_train_data.pckl', 'wb') as hist:\n",
    "    \n",
    "    pickle.dump(history_DeepLabV3Plus.history,hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "plt.plot(history_DeepLabV3Plus.history['loss'])\n",
    "\n",
    "plt.plot(history_DeepLabV3Plus.history['val_loss'])\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.ylabel('Binary Cross Entropy')\n",
    "\n",
    "plt.legend(['Train','Validation'])\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "plt.plot(history_DeepLabV3Plus.history['accuracy'])\n",
    "\n",
    "plt.plot(history_DeepLabV3Plus.history['val_accuracy'])\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.legend(['Train','Validation'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_resampled__=apply_pca_transform(test_resampled)\n",
    "\n",
    "patch_size=32\n",
    "\n",
    "predicted_segments_DLab=model_DeepLabV3Plus.predict(test_resampled__)\n",
    "\n",
    "print(predicted_segments_DLab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_segments_DLab[predicted_segments_DLab>0.6]=1\n",
    "\n",
    "predicted_segments_DLab[predicted_segments_DLab<=0.6]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "\n",
    "#map_r,map_c=992,2038\n",
    "\n",
    "#map_r,map_c=263, 615\n",
    "\n",
    "#map_r,map_c=301,301\n",
    "\n",
    "map_r,map_c=gt.shape[0],gt.shape[1]\n",
    "\n",
    "patch_size=13\n",
    "\n",
    "num_r,num_c=int(map_r/patch_size),int(map_c/patch_size)\n",
    "\n",
    "print(predicted_segments_DLab.shape)\n",
    "\n",
    "for ii in range(num_r):\n",
    "    \n",
    "    if ii==0:\n",
    "\n",
    "        initial=np.hstack(predicted_segments_DLab[ii*num_c:ii*num_c+num_c,:,:])\n",
    "    else:\n",
    "        \n",
    "        initial=np.vstack((initial,np.hstack(predicted_segments_DLab[ii*num_c:ii*num_c+num_c,:,:]))) \n",
    "        \n",
    "DLab_map=initial.astype(int)\n",
    "        \n",
    "#print(DLab_map.shape)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.imshow(DLab_map)\n",
    "\n",
    "plt.axis('off')\n",
    "    \n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('D:/Nafiseh/flood_proposal/DeepLabV3_Plus_gatineau.npy',DLab_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "\n",
    "#gt=mask_union_test\n",
    "\n",
    "change_map=DLab_map\n",
    "\n",
    "change_map_r=zoom(change_map,[gt.shape[0]/change_map.shape[0],gt.shape[1]/change_map.shape[1],1],order=0)\n",
    "\n",
    "sns.heatmap(confusion_matrix(np.ravel(gt),np.ravel(change_map_r)),annot=True,fmt='d')\n",
    "\n",
    "plt.savefig('D:/Nafiseh/flood_proposal/confusion_matrix_deep_labv3_plus_RCM_Abbotsford_suburban_new_train_data.tif')\n",
    "\n",
    "text_file=open('D:/Nafiseh/flood_proposal/classification_report_deep_labv3_plus_RCM_Abbotsford_suburban_new_train_data.txt','a')\n",
    "\n",
    "print(classification_report(np.ravel(gt),np.ravel(change_map_r)),file=text_file)\n",
    "\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
